# -*- coding: utf-8 -*-
import os
import glob
import re
import pickle

try:
    from .converter import DataConverter
    from .physics import PhysicsEngine
    from .processor import DataProcessor
except ImportError:
    import sys
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from converter import DataConverter
    from physics import PhysicsEngine
    from processor import DataProcessor

class DataLoader:
    TARGET_SOURCES = ['pressure', 'vibration', 'hsc']

    def __init__(self, series_config, output_dir=None):
        self.series_config = series_config
        settings = series_config.get('settings', {})
        self.base_dir = settings.get('base_dir') or series_config.get('base_dir', '.')
        self.sources = series_config.get('sources', {})
        
        self.converter = DataConverter()
        self.physics = PhysicsEngine()
        self.processor = DataProcessor()
        
        self.results_root = output_dir if output_dir else os.path.join(self.base_dir, "033_è§£æžçµæžœ")

    def load_shot_data(self, spec_config, force_reload=False):
        shot_number = spec_config['shot_number']
        measurements = spec_config.get('measurements', [])
        processing_config = spec_config.get('processing', {})
        
        # â˜…ä¿®æ­£: spec.yaml ã‹ã‚‰ acquisition è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        acquisition_config = spec_config.get('acquisition', {})
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        default_sr = float(acquisition_config.get('sampling_rate', 1000.0))
        # â˜…é‡è¦: ãƒˆãƒªã‚¬ãƒ¼å‰æ™‚é–“ (ä¾‹: -0.1) ã‚’å–å¾—
        start_time_offset = float(acquisition_config.get('start_time', 0.0))
        
        data_store = {}

        for source_name, source_info in self.sources.items():
            if source_name not in self.TARGET_SOURCES: continue

            cache_root = self._get_cache_directory(source_name)
            os.makedirs(cache_root, exist_ok=True)
            
            print(f"[Loader] ã‚½ãƒ¼ã‚¹: '{source_name}'")

            # === HSC ===
            if source_name == 'hsc':
                # HSCã¯ hsc_analyzer å´ã§ pre_trigger_frames ã‹ã‚‰ start_time ã‚’è¨ˆç®—æ¸ˆã¿
                hsc_pkl_path = os.path.join(cache_root, f"shot{shot_number:03d}_hsc.pkl")
                
                if os.path.exists(hsc_pkl_path):
                    try:
                        with open(hsc_pkl_path, 'rb') as f:
                            hsc_data = pickle.load(f)
                            if isinstance(hsc_data, dict):
                                data_store.update(hsc_data)
                                print(f"  -> HSCãƒ‡ãƒ¼ã‚¿çµåˆ: {len(hsc_data)} items")
                            else:
                                print(f"  âš ï¸ HSCã‚­ãƒ£ãƒƒã‚·ãƒ¥å½¢å¼ä¸ä¸€è‡´ (ã‚¹ã‚­ãƒƒãƒ—)")
                    except Exception as e:
                        print(f"  âš ï¸ HSCãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                else:
                    print(f"  â„¹ï¸  HSCã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã— (æœªè§£æž): {os.path.basename(hsc_pkl_path)}")
                continue

            # === CSVç³»ã‚»ãƒ³ã‚µ (Pressure, Vibration) ===
            folder_name = source_info.get('folder')
            hint = source_info.get('hint', None)
            target_dir = os.path.join(self.base_dir, folder_name)
            
            csv_path = self._smart_find_file(target_dir, shot_number, hint=hint)
            if not csv_path:
                print(f"  âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãªã—: Shot {shot_number} in {folder_name}")
                continue
            
            file_base = os.path.splitext(os.path.basename(csv_path))[0]
            cache_path = os.path.join(cache_root, file_base + ".pkl")

            loaded_dict = None
            if not force_reload and self._is_cache_valid(csv_path, cache_path):
                try:
                    with open(cache_path, 'rb') as f:
                        temp_data = pickle.load(f)
                        if isinstance(temp_data, dict):
                            # â˜…ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã® start_time ãŒ spec ã¨åˆã£ã¦ã„ã‚‹ã‹ç¢ºèªã™ã‚‹ã®ã¯é›£ã—ã„ã®ã§
                            # specã®å€¤ã§ä¸Šæ›¸ãã™ã‚‹å‡¦ç†ã‚’å…¥ã‚Œã‚‹ã¨ã‚ˆã‚Šå®‰å…¨ã ãŒã€ä»Šå›žã¯Converterå†å®Ÿè¡Œã§å¯¾å¿œ
                            loaded_dict = temp_data
                        else:
                            print(f"  ðŸ”„ å¤ã„å½¢å¼ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œå‡º -> å†ç”Ÿæˆã—ã¾ã™")
                except: pass
            
            if loaded_dict is None:
                # â˜…ä¿®æ­£: start_time ã¨ sampling_rate ã‚’æ¸¡ã™
                save_path = self.converter.process(
                    csv_path=csv_path, 
                    output_dir=cache_root, 
                    sensor_configs=measurements, 
                    processing_config=processing_config,
                    default_sampling_rate=default_sr,
                    default_start_time=start_time_offset # â† ã“ã‚ŒãŒé‡è¦
                )
                if save_path:
                    with open(save_path, 'rb') as f:
                        loaded_dict = pickle.load(f)

            if loaded_dict and isinstance(loaded_dict, dict):
                data_store.update(loaded_dict)
                print(f"  -> çµåˆ: {len(loaded_dict)} items from {source_name}")

        if not data_store:
            print("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return {}

        # å¾Œå‡¦ç†
        pre_pipeline = spec_config.get('pre_processing', [])
        if pre_pipeline: self.processor.apply_preprocessing(data_store, pre_pipeline)

        derived = spec_config.get('derived_channels', {})
        self.physics.add_derived_channels(data_store, derived)

        post_pipeline = spec_config.get('post_processing', [])
        if post_pipeline: self.processor.apply_preprocessing(data_store, post_pipeline)

        return data_store

    def _get_cache_directory(self, source_name):
        base_cache_dir = os.path.join(self.results_root, ".cache")
        if source_name == 'vibration': dir_name = "vibration"
        elif source_name == 'hsc': dir_name = "hsc_brightness"
        else: dir_name = source_name
        return os.path.join(base_cache_dir, dir_name)

    def _smart_find_file(self, search_dir, shot_num, hint=None):
        if not os.path.exists(search_dir): return None
        files = glob.glob(os.path.join(search_dir, "*.csv"))
        candidates = [f for f in files if int(shot_num) in [int(n) for n in re.findall(r'\d+', os.path.basename(f))]]
        if not candidates: return None
        if len(candidates) > 1 and hint:
            filtered = [c for c in candidates if hint.lower() in os.path.basename(c).lower()]
            if filtered: return filtered[0]
        return candidates[0]

    def _is_cache_valid(self, source, cache):
        if not os.path.exists(cache): return False
        return os.path.getmtime(source) < os.path.getmtime(cache)